{"cells":[{"cell_type":"markdown","metadata":{"id":"F24pbMKw8uT9"},"source":["<h2>개인 구글 드라이브와 colab 연동</h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4X-lNQfT1D1v","outputId":"16453d05-25ac-4032-fa52-48ea6a6d5b68","executionInfo":{"status":"ok","timestamp":1715264235742,"user_tz":-540,"elapsed":20677,"user":{"displayName":"이주형","userId":"07579585688851948048"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/gdrive\", force_remount=True)"]},{"cell_type":"code","source":["!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6sHCK4trEaG","executionInfo":{"status":"ok","timestamp":1715264245104,"user_tz":-540,"elapsed":9365,"user":{"displayName":"이주형","userId":"07579585688851948048"}},"outputId":"f43e2e9e-3e8b-4c77-bc2a-2abdd4d68f77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kobert_tokenizer\n","  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-5m9osowk/kobert-tokenizer_808613449240496ea39525a78c9d3375\n","  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-5m9osowk/kobert-tokenizer_808613449240496ea39525a78c9d3375\n","  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: kobert_tokenizer\n","  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4633 sha256=f4fb5f894cc4557a0a835c7f4b5a4be6f16ecbe0af7274328534c7bfc48ca8c8\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-qw7z7iiy/wheels/e9/1a/3f/a864970e8a169c176befa3c4a1e07aa612f69195907a4045fe\n","Successfully built kobert_tokenizer\n","Installing collected packages: kobert_tokenizer\n","Successfully installed kobert_tokenizer-0.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21270,"status":"ok","timestamp":1715264266369,"user":{"displayName":"이주형","userId":"07579585688851948048"},"user_tz":-540},"id":"FVnmDn36Gh2D","outputId":"21e3fb2b-5e69-4ed3-a40d-6253010912f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"]}],"source":["!pip install transformers\n","!pip install sentencepiece\n","root_dir = \"/gdrive/MyDrive/NLP/week10/\"\n","\n","import sys\n","sys.path.append(root_dir)"]},{"cell_type":"markdown","metadata":{"id":"YfWZLECjoJkn"},"source":["<h2>WSD 모델</h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o5861idd25QB"},"outputs":[],"source":["from transformers import BertPreTrainedModel, BertModel\n","#BERT 사전학습모델을 사용\n","\n","class WSD(BertPreTrainedModel):\n","#BERT 사전학습모델을 상속\n","    def __init__(self, config):\n","      #class의 attribute를 선언 및 초기화하는 작업\n","        super(WSD, self).__init__(config)\n","\n","        # BERT 모델\n","        self.bert = BertModel(config)\n","\n","    def forward(self, input_ids, attention_mask):\n","      #attention_mask는 Self Attention 범위 지정을 위한 마스크(padding 부분이 아닌 곳에만 attention을 걸기)\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        #기존에 정의한 self.bert 모델에 파라미터를(입력의 token과 attention_mask) 대입해 outputs을 받기\n","\n","        # BERT 출력 (batch_size, max_length, hidden_size)\n","        bert_output = outputs[0]\n","        #우리는 bert 모델의 batch 중에서 맨 마지막 층을 사용할 것\n","        #즉, output들 중에서 내가 궁금한 '배'라는 word를 원하면 배의 embedding vector를 뽑아서 사용한다는 의\n","        return bert_output"]},{"cell_type":"markdown","metadata":{"id":"caSL4RyA1OOm"},"source":["<h2>데이터 읽고 전처리 하기</h2>\n","\n","<pre>\n","<b>1. read_data(file_path)</b>\n","  \"datas.txt\" 파일을 읽기 위한 함수\n","  \n","  데이터 예시)\n","    토큰들로 구성된 토큰 시퀀스 \\t \"배\"에 대응하는 토큰 인덱스\n","\n","    ▁보기 만 ▁해도 ▁배 가 ▁부르 다 .\t3\n","    ▁점 심을 ▁먹 지 ▁못해 ▁배 가 ▁많이 ▁고 팠 다 .\t5\n","    ▁배 ▁한 ▁ 척 이 ▁바다 ▁한 가 운 데 ▁떠 ▁있다 .\t0\n","    ▁그 ▁섬 에는 ▁하루 에 ▁두 ▁번 씩 ▁배 가 ▁들어 온 다 .\t8\n","    ▁나는 ▁ 과 일 ▁중 에서 ▁배 를 ▁가장 ▁좋아 한다 .\t6\n","    ▁사 각 사 각 ▁ 씹 히 는 ▁배 의 ▁맛 이 ▁달 고 ▁시 원 하다 .\t8\n","  \n","  read_data(file_path)\n","  args\n","    file_path : 읽고자 하는 데이터의 경로\n","  return\n","    토큰 sequence, \"배\"에 대응하는 토큰 인덱스를 담고 있는 리스트\n","    \n","    출력 예시)\n","      datas = [ (['▁보기', '만', '▁해도', '▁배', '가', '▁부르', '다', '.'], 3)\n","\n","                (...),\n","        \n","              ]\n","      \n","<b>2. convert_data2feature(datas, max_length, tokenizer)</b>\n","  입력 데이터를 indexing 한 후, padding 추가\n","  Tensor로 변환\n","   \n","  convert_data2feature(datas, max_length, tokenizer)\n","  args\n","    datas : 토큰 sequence, \"배\"에 대응하는 토큰 인덱스를 담고 있는 리스트\n","    max_length : 입력의 최대 길이\n","    tokenizer : BERT 토크나이저\n","  return\n","    input_ids_features : 입력 문장에 대한 index sequence\n","    attention_mask_features : padding을 제외한 나머지 실제 토큰 정보를 갖고 있는 sequence\n","    \n","  전처리 예시)\n","    tokens : ['[CLS]', '▁보기', '만', '▁해도', '▁배', '가', '▁부르', '다', '.', '[SEP]']\n","    input_ids : [2, 2362, 6150, 5002, 2287, 5330, 2432, 5782, 54, 3, ...]\n","    attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...]\n"," </pre>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iOGS8rse1ZZZ"},"outputs":[],"source":["import torch\n","\n","\n","def read_data(file_path):\n","  #입력 문장 데이터가 Bert의 토크나이저에 의해 '_보기 만 _해도'같이 토크나이징 되어 있는데\n","  #이 토큰 형태를 sentencepiece라고 하고 이것이 bert가 사용하는 토크나이저가 잘라놓은 형태\n","  #sentencepiece는 BytePairEncoding이라는 방법으로 잘라놓은 것들을 wordPiece라고 하는데 이와 매우 유사한 방식\n","  #이런 식으로 자른 것을 입력으로 받을 것\n","  #물론 '_점 심을 +먹 지'처럼 형태소 분석을 하지 않고 통계적으로 wordPiece방식으로 잘라서 이상하게 잘렸는데\n","  #장점은 세상의 어떤 언어든지 다 이런 방식으로 자를 수 있음.\n","  #즉, 여러 나라의 언어들을 쉽게 의미 단위로 비슷하게 자를 수 있다는 장점이 있기에 구글이 사용함\n","\n","  #'▁점 심을 ▁먹 지 ▁못해 ▁배 가 ▁많이 ▁고 팠 다 .    5'을 보면 \\t으로 분리되어 뒤에 숫자가 있는데\n","  #이는 타겟으로 하는 그 단어 즉, embedding을 뽑아보고 싶은 그 단어의 위치를 의미\n","  #즉, 0부터 시작해 5번째인 '_배'를 명시적으로 의미\n","    with open(file_path, \"r\", encoding=\"utf8\") as inFile:\n","        lines = inFile.readlines()\n","\n","    datas = []\n","    for line in lines:\n","        # 입력 데이터를 \\t을 기준으로 분리\n","        pieces = line.strip().split(\"\\t\")\n","\n","        # 입력 토큰 시퀀스(자른 word의 토큰), 목표 토큰 인덱스(내가 원하는 word 토큰의 위치)\n","        token_sequence, target_token_index = pieces[0].split(\" \"), int(pieces[1])\n","\n","        datas.append((token_sequence, target_token_index))\n","        #튜플 단위로 묶어서 리스트에 append\n","    return datas\n","\n","\n","def convert_data2feature(datas, max_length, tokenizer):\n","  #datas : 위에서 만든 튜플 단위 리스트\n","  #max_length : bert가 입력받을 수 있는 최대 길이 (보통 512)\n","  #tokenizer : senetencePiece 단위로 쪼개주는 토크나이저\n","\n","    input_ids_features, attention_mask_features = [], []\n","    #input_ids_features는 input_ids인 embedding된 token을 담기\n","    #attention_mask_features는 input_ids_features가 유효한 것들에만 1로 마스킹해서 담기\n","\n","    #(입력 토큰 시퀀스(자른 word의 토큰), 목표 토큰 인덱스(내가 원하는 word 토큰의 위치))\n","    for token_sequence, target_token_index in datas:\n","\n","        # CLS, SEP 토큰 추가\n","        tokens = [tokenizer.cls_token]  #Bert의 입력 처음에는 [CLS] 토큰을 넣어야 함\n","        tokens += token_sequence        #자른 word의 토큰을 차례대로 대입\n","        tokens = tokens[:max_length - 1]#너무 입력이 많으면 잘라야 하고 [SEP] 토큰 자리도 준비해야함\n","        tokens += [tokenizer.sep_token] #마지막은 [SEP] 토큰을 대입\n","\n","        # word piece들을 대응하는 index로 치환\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","        #bert 내의 tokenizer를 그대로 사용해서 토큰을 넣으면 id들을 그 결과로 반환\n","\n","        # padding을 제외한 실제 데이터 정보를 반영해주기 위한 attention mask\n","        attention_mask = [1] * len(input_ids)\n","        #내가 받은 id의 길이만큼 1을 넣고 나머지는 padding으로 0으로 attention_mask를 처리\n","        #이 방식은 attention_mask에도 동\n","\n","        # padding 생성\n","        padding = [tokenizer._convert_token_to_id(tokenizer.pad_token)] * (max_length - len(input_ids))\n","        padding_for_mask = [0] * (max_length - len(input_ids))\n","\n","        # padding 추가\n","        input_ids += padding\n","        attention_mask += padding_for_mask\n","\n","        # 변환한 데이터를 각 리스트에 저장\n","        input_ids_features.append(input_ids)\n","        attention_mask_features.append(attention_mask)\n","\n","    # 변환한 데이터를 Tensor 객체에 담아 반환\n","    input_ids_features = torch.tensor(input_ids_features, dtype=torch.long)\n","    attention_mask_features = torch.tensor(attention_mask_features, dtype=torch.long)\n","\n","    return input_ids_features, attention_mask_features"]},{"cell_type":"markdown","metadata":{"id":"urLIOIHT779c"},"source":["<h2>WSD 모델을 사용하여 문맥에 따라 변하는 단어 벡터 확인</h2>\n","\n","<pre>\n","<b>1. read_data(file_path) 함수를 사용하여 입력 데이터 읽기</b>\n","\n","<b>2. convert_data2feature(datas, max_length, tokenizer) 함수를 사용하여 데이터 전처리</b>\n","\n","<b>3. WSD 모델을 활용하여 \"배\"에 대응하는 벡터 표현 추출</b>\n","\n","<b>4. 서로 다른 문장에서 사용된 \"배\"에 대응하는 벡터 표현 사이의 유사도 비교</b>\n","</pre>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsYLc2YK8eNc"},"outputs":[],"source":["import torch\n","import operator\n","import numpy as np\n","\n","from transformers import BertConfig\n","#from tokenization_kobert import KoBertTokenizer\n","from kobert_tokenizer import KoBERTTokenizer\n","\n","def get_cos_sim(vector_1, vector_2):\n","  #input_id를 vector한 뒤에 그것으로 코사인 유사도를 내적으로 계산해 반환\n","  return np.dot(vector_1, vector_2)/(np.linalg.norm(vector_1)*np.linalg.norm(vector_2))\n","\n","#우리는 training할 것이 아니라 사전학습모델에 넣어 결과만 뽑고 싶기에 test만 정\n","def test(config):\n","    # BERT config 객체 생성 : 사전학습모델의 것을 그대로 사용\n","    bert_config = BertConfig.from_pretrained(pretrained_model_name_or_path=config[\"pretrained_model_name_or_path\"],\n","                                             cache_dir=config[\"cache_dir_path\"])\n","\n","    # BERT tokenizer 객체 생성 : tokenizer 객체도 사전학습모델의 것을 그대로 이용\n","    bert_tokenizer = KoBERTTokenizer.from_pretrained(pretrained_model_name_or_path=config[\"pretrained_model_name_or_path\"],\n","                                                     cache_dir=config[\"cache_dir_path\"])\n","\n","    # 데이터 읽기 : sentencePiece로 잘려진 data 그대로 return\n","    datas = read_data(file_path=config[\"data_path\"])\n","\n","    # 데이터 전처리 : 받은 data를 넣어서 input의 id와 attention_mask를 받\n","    input_ids_features, attention_mask_features = convert_data2feature(datas=datas,\n","                                                                       max_length=config[\"max_length\"],\n","                                                                       tokenizer=bert_tokenizer)\n","\n","    # 사전 학습된 BERT 모델 파일로부터 가중치 불러옴\n","    # 우리가 만든 WSD는 사전학습모델인 BERT를 상속받음\n","    model = WSD.from_pretrained(pretrained_model_name_or_path=config[\"pretrained_model_name_or_path\"],\n","                                cache_dir=config[\"cache_dir_path\"], config=bert_config).cuda()\n","\n","    input_ids_features = input_ids_features.cuda()\n","    attention_mask_features = attention_mask_features.cuda()\n","\n","    # 모델 예측 결과\n","    bert_outputs = model(input_ids_features, attention_mask_features)\n","    #결과로서 output[0]인 마지막 층의 임베딩 값들이 나옴 : [CLS] ~ [PAD]\n","\n","    #눈으로 보기 편하게 하기 위해 tensor형태에서 list 형태로 변\n","    input_ids_features = input_ids_features.cpu().detach().numpy().tolist()\n","    bert_outputs = bert_outputs.cpu().detach().numpy().tolist()\n","\n","    # batch 단위로 구성되어 있어 반복문을 통해 하나씩 확인\n","    word2vec = {}\n","    # batch 안에 사람의 배, 타는 배, 먹는 배가 각각 2개씩 있는데 거기서 for문으로 하나씩 뽑기\n","    for batch_index in range(len(bert_outputs)):#0~5번 인덱스까지 6회 반복\n","        input_tokens = bert_tokenizer.convert_ids_to_tokens(input_ids_features[batch_index])\n","        bert_output = bert_outputs[batch_index]\n","\n","        #batch 내에서 인덱스로 입력 토큰 시퀀스 문장인 token_sequence를 받고 내가 원하는 '배'의 token_index를 반환 받기\n","        token_sequence, target_token_index = datas[batch_index]\n","\n","        # token_sequence : 입력 토큰 시퀀스 문장으로 변환\n","        # ['▁보기', '만', '▁해도', '▁배', '가', '▁부르', '다', '.'] -> 보기만 해도 배가 부르다.\n","        # 이는 그저 보기 좋게 하기 위해서 구성한 구문 & 메서드\n","        sentence = bert_tokenizer.convert_tokens_to_string(token_sequence)\n","\n","        # token_sequence 앞에 [CLS] 토큰이 추가되었기 때문에 1 추가\n","        target_token_index += 1\n","\n","        target_token = input_tokens[target_token_index]\n","        # 토큰을 단어로 변경 (_배 -> 배)\n","        # 이는 그저 보기 좋게 하기 위해서 구성한 구문 & 메서드\n","        target_word = bert_tokenizer.convert_tokens_to_string([target_token])\n","\n","        #목표하는 토큰 인덱스를 output 리스트에 대입해 '배'에 해당하는 embedding vector를 반환받기\n","        target_vector = bert_output[target_token_index]\n","        # 단어, 단어가 사용된 batch_index, 단어가 사용된 문장\n","        # 배_1 (보기만 해도 배가 부르다.)\n","        word2vec[\"{}_{} ({})\".format(target_word, batch_index+1, sentence)] = target_vector\n","        #word2vec이라는 딕셔너리의 key에는 '배_i'와 그것에 해당하는 '문장'을 넣고\n","        #그 딕셔너리에 key의 value로 그 target_word의 embedding 값인 target_vector 넣기\n","\n","    # \"배_i\"에 대응하는 나머지 모든 벡터 표현들 사이의 유사도 계산\n","    word_similarity = {}\n","    for word_1, vector_1 in word2vec.items():\n","      #딕셔너리의 key와 value를 하나씩 가져와서\n","\n","        # word_1과 나머지 단어들 사이의 유사도를 담을 리스트 생성\n","        #즉, 해당 key를 기준으로 다른 key들 간의 유사도를 담을 리스트\n","        word_similarity[word_1] = []\n","        for word_2, vector_2 in word2vec.items():\n","            #같은 토큰이면 건너뛰기\n","            if word_1 == word_2:\n","              continue\n","\n","            # word1과 word2 사이의 코사인 유사도를 계산한 후, 그 결과를 word_similarity 딕셔너리에 저장\n","            # word_similarity의 key : word1, value : (word2, 유사도)\n","            #           코드 작성 부분              #\n","            cos_sim = get_cos_sim(vector_1 = vector_1, vector_2 = vector_2)\n","            word_similarity[word_1].append((word_2, cos_sim))\n","            #word_1 기준 word_2~word_6까지 각각의 유사도를 튜플로 엮어 value에 append\n","\n","    print(\"\\n단어1 (단어1이 사용된 문장) vs 단어2 (단어2가 사용된 문장) -> 유사도\\n\")\n","    for word in word_similarity.keys():\n","\n","        #각 key에 대해 value인 유사도를 기준으로 정렬, reverse=True를 통해 내림차순으로 정렬\n","        word_similarity[word] = sorted(word_similarity[word], key=operator.itemgetter(1), reverse=True)\n","\n","        for index in range(len(word_similarity[word])):\n","            print(\"{} vs {} -> {}\".format(word, word_similarity[word][index][0], round(word_similarity[word][index][1], 4)))\n","        print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbtyjwvtFxf7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715264326581,"user_tz":-540,"elapsed":1937,"user":{"displayName":"이주형","userId":"07579585688851948048"}},"outputId":"c30993b5-7ccf-42fd-ab24-2ebc569421d5"},"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n","The class this function is called from is 'KoBERTTokenizer'.\n"]},{"output_type":"stream","name":"stdout","text":["\n","단어1 (단어1이 사용된 문장) vs 단어2 (단어2가 사용된 문장) -> 유사도\n","\n","배_1 (보기만 해도 배가 부르다.) vs 배_2 (점심을 먹지 못해 배가 많이 고팠다.) -> 0.7238\n","배_1 (보기만 해도 배가 부르다.) vs 배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) -> 0.6015\n","배_1 (보기만 해도 배가 부르다.) vs 배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) -> 0.5815\n","배_1 (보기만 해도 배가 부르다.) vs 배_5 (나는 과일 중에서 배를 가장 좋아한다.) -> 0.5376\n","배_1 (보기만 해도 배가 부르다.) vs 배_3 (배 한 척이 바다 한가운데 떠 있다.) -> 0.4839\n","\n","배_2 (점심을 먹지 못해 배가 많이 고팠다.) vs 배_1 (보기만 해도 배가 부르다.) -> 0.7238\n","배_2 (점심을 먹지 못해 배가 많이 고팠다.) vs 배_5 (나는 과일 중에서 배를 가장 좋아한다.) -> 0.5074\n","배_2 (점심을 먹지 못해 배가 많이 고팠다.) vs 배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) -> 0.5071\n","배_2 (점심을 먹지 못해 배가 많이 고팠다.) vs 배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) -> 0.4337\n","배_2 (점심을 먹지 못해 배가 많이 고팠다.) vs 배_3 (배 한 척이 바다 한가운데 떠 있다.) -> 0.4213\n","\n","배_3 (배 한 척이 바다 한가운데 떠 있다.) vs 배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) -> 0.7059\n","배_3 (배 한 척이 바다 한가운데 떠 있다.) vs 배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) -> 0.6008\n","배_3 (배 한 척이 바다 한가운데 떠 있다.) vs 배_5 (나는 과일 중에서 배를 가장 좋아한다.) -> 0.5345\n","배_3 (배 한 척이 바다 한가운데 떠 있다.) vs 배_1 (보기만 해도 배가 부르다.) -> 0.4839\n","배_3 (배 한 척이 바다 한가운데 떠 있다.) vs 배_2 (점심을 먹지 못해 배가 많이 고팠다.) -> 0.4213\n","\n","배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) vs 배_3 (배 한 척이 바다 한가운데 떠 있다.) -> 0.7059\n","배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) vs 배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) -> 0.6077\n","배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) vs 배_1 (보기만 해도 배가 부르다.) -> 0.6015\n","배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) vs 배_5 (나는 과일 중에서 배를 가장 좋아한다.) -> 0.5753\n","배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) vs 배_2 (점심을 먹지 못해 배가 많이 고팠다.) -> 0.5071\n","\n","배_5 (나는 과일 중에서 배를 가장 좋아한다.) vs 배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) -> 0.749\n","배_5 (나는 과일 중에서 배를 가장 좋아한다.) vs 배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) -> 0.5753\n","배_5 (나는 과일 중에서 배를 가장 좋아한다.) vs 배_1 (보기만 해도 배가 부르다.) -> 0.5376\n","배_5 (나는 과일 중에서 배를 가장 좋아한다.) vs 배_3 (배 한 척이 바다 한가운데 떠 있다.) -> 0.5345\n","배_5 (나는 과일 중에서 배를 가장 좋아한다.) vs 배_2 (점심을 먹지 못해 배가 많이 고팠다.) -> 0.5074\n","\n","배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) vs 배_5 (나는 과일 중에서 배를 가장 좋아한다.) -> 0.749\n","배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) vs 배_4 (그 섬에는 하루에 두 번씩 배가 들어온다.) -> 0.6077\n","배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) vs 배_3 (배 한 척이 바다 한가운데 떠 있다.) -> 0.6008\n","배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) vs 배_1 (보기만 해도 배가 부르다.) -> 0.5815\n","배_6 (사각사각 씹히는 배의 맛이 달고 시원하다.) vs 배_2 (점심을 먹지 못해 배가 많이 고팠다.) -> 0.4337\n","\n"]}],"source":["import os\n","\n","\n","if(__name__==\"__main__\"):\n","    cache_dir = os.path.join(root_dir, \"cache\")\n","    if not os.path.exists(cache_dir):\n","        os.makedirs(cache_dir)\n","\n","    config = {\n","        \"data_path\": os.path.join(root_dir, \"datas.txt\"),\n","        \"cache_dir_path\": cache_dir,\n","        \"pretrained_model_name_or_path\": \"skt/kobert-base-v1\",\n","        \"max_length\": 30\n","    }\n","\n","    test(config=config)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}